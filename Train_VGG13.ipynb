{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np  \n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,ZeroPadding2D,BatchNormalization,Activation,add,AveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 26880 Train Images\n"
     ]
    }
   ],
   "source": [
    "dataset = 'UCM'\n",
    "base_path='./Dataset/' + dataset\n",
    "ModelName = \"VGG13\"\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "\n",
    "X_Train = np.load(base_path + \"/X_Train_224.npy\");\n",
    "Y_Train = np.load(base_path + \"/Y_Train.npy\");\n",
    "\n",
    "print(\"Get %d Train Images\" %(len(X_Train)))       \n",
    "num_classes = Y_Train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG13(shape):\n",
    "    model = Sequential(name=ModelName)  \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=shape,padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model.add(Flatten())  \n",
    "    model.add(Dense(4096,activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(4096,activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(num_classes,activation='softmax'))  \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     49280     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                86037     \n",
      "=================================================================\n",
      "Total params: 129,012,309\n",
      "Trainable params: 129,012,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG13(X_Train.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "26880/26880 [==============================] - 169s 6ms/step - loss: nan - accuracy: 0.0475 2s - \n",
      "Epoch 2/4\n",
      " 8128/26880 [========>.....................] - ETA: 1:57 - loss: nan - accuracy: 0.0449"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_Train, Y_Train, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "save_folder = 'models/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + dataset\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "# serialize model to JSON\n",
    "import pickle\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_folder, ModelName + \".json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save(os.path.join(save_folder, ModelName + \".h5\"))\n",
    "#pickle.dump(history.history, open('history/UCMerced_LandUse/AlexNet.p','wb'))\n",
    "print(\"Saved Model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "outputFileName = []\n",
    "\n",
    "save_folder2 = 'FeatureMap/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + ModelName + '/' + dataset  +'/'\n",
    "\n",
    "for i in range(0, num_classes):\n",
    "    file_dir = os.path.join(base_path, 'Test', All_Labels[i])\n",
    "    file_names = os.listdir(file_dir)\n",
    "    for file_name in file_names:\n",
    "        if(file_name.endswith(Extension)):\n",
    "            finalFileName = os.path.join(file_dir, file_name)\n",
    "            Label = np.linspace(0, 0, num_classes, dtype='int32')\n",
    "            Label[i] = 1    \n",
    "            images.append(read_image(finalFileName))\n",
    "            labels.append(Label)\n",
    "            outputFileName.append(os.path.join(save_folder2, All_Labels[i], file_name).replace(Extension, \".txt\"))\n",
    "\n",
    "print(\"Get %d Test Images\" %(len(images)))\n",
    "output = np.array(images, dtype=\"float32\")\n",
    "output = preprocess_input(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder2):\n",
    "    os.makedirs(save_folder2)\n",
    "    for Label in All_Labels:        \n",
    "        os.makedirs(os.path.join(save_folder2, Label))\n",
    "        \n",
    "from keras.models import Model  \n",
    "OutPutLayer = Model(inputs=model.input, outputs=model.get_layer('dense_2').output)\n",
    "print(\"Saved FeatureMap to disk...\")\n",
    "for i in range(0, len(output)):\n",
    "    p = OutPutLayer.predict(output[i : i + 1])    \n",
    "    np.savetxt(outputFileName[i : i + 1][0], p)\n",
    "    #file_object = open(outputFileName[i : i + 1][0], 'w')\n",
    "    #file_object.write(str(p))\n",
    "    #file_object.close()\n",
    "    print(\"\\r当前输出：%d\" %(i + 1), end= \" \")\n",
    "\n",
    "print(\"\\n保存完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count = 0\n",
    "for i in range(0, len(output)):\n",
    "    p=model.predict(output[i : i + 1])\n",
    "    if np.argmax(p)==np.argmax(labels[i : i + 1][0]):\n",
    "        Count += 1\n",
    "    \n",
    "print(Count/len(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
